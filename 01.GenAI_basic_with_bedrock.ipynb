{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "658ffbb7",
   "metadata": {},
   "source": [
    "# 생성형 AI 기본 API 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b4a87",
   "metadata": {},
   "source": [
    "## 1. 미리 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ab366",
   "metadata": {},
   "source": [
    "### 1.1 프로그램 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치\n",
    "! pip install --upgrade boto3  > /dev/null 2>&1\n",
    "! pip list | egrep 'boto3' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646da282",
   "metadata": {},
   "source": [
    "### 1.2 인증 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9985fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import json\n",
    "\n",
    "# 프로그램의 시간측정을 위한 Class 선언 \n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        print(\"\\n\\n-----------\")\n",
    "        print(f\"모델 실행 시간: {time.time() - self.start:.2f}초 \\n\")\n",
    "        \n",
    "# AWS 자격 증명 확인\n",
    "try:\n",
    "    session = boto3.Session()\n",
    "    credentials = session.get_credentials()\n",
    "    print(\"AWS 자격 증명 확인 완료\")\n",
    "    print(f\"리전: {session.region_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"AWS 자격 증명 오류: {e}\")\n",
    "    print(\"AWS CLI를 사용하여 자격 증명을 설정해주세요: aws configure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b1b68",
   "metadata": {},
   "source": [
    "### 1.3 Bedrock 클라이언트 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = session.client('bedrock-runtime')\n",
    "bedrock2 = session.client('bedrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d8296e",
   "metadata": {},
   "source": [
    "### 1.4 Bedrock이 제공하는 모델 확인 하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fbaadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = bedrock2.list_foundation_models()\n",
    "\n",
    "print(\"=== 모델 상세 정보 ===\")\n",
    "for model in response['modelSummaries']:\n",
    "    print(f\"모델 ID: {model['modelId']}\")\n",
    "    print(f\"모델 ARN: {model['modelArn']}\")\n",
    "    print(f\"제공업체: {model['providerName']}\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee92a5",
   "metadata": {},
   "source": [
    "### 1.5 이 프로그램에서 사용할 기본 모델 선언 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5baa614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 리전 확인\n",
    "current_region = boto3.Session().region_name\n",
    "\n",
    "# 현재 리전에 맞는 Model ID 자동 검색\n",
    "models = bedrock2.list_foundation_models()['modelSummaries']\n",
    "\n",
    "base_model=\"\"\n",
    "# 버지니아 리전인 경우 Cross-region inference 사용\n",
    "if current_region == 'us-east-1':\n",
    "    base_model = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "else:\n",
    "    find_models = next((m for m in models if 'claude-3-5-haiku' in m['modelId'].lower()), None)\n",
    "    base_model = find_models['modelId']\n",
    "\n",
    "print(\"[Model ID] claude-3-5-haiku :\", base_model)\n",
    "print(\"[Current Region]:\", current_region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dec011",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Bedrock Invoke API 호출 \n",
    "- https://docs.aws.amazon.com/bedrock/latest/userguide/inference-invoke.html \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca69ea",
   "metadata": {},
   "source": [
    "### 2.1 Claude model 용 Helper함수 선언후 모델 호출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e7875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_invokeapi_claude(message, system=\"\", history=None ,display=True):\n",
    "    if history:\n",
    "        history.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]})\n",
    "        body = json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 1500,\n",
    "            \"temperature\": 0.5,\n",
    "            \"system\": system,\n",
    "            \"messages\": history\n",
    "            })\n",
    "    else:\n",
    "        body = json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 1500,\n",
    "            \"temperature\": 0.5,\n",
    "            \"system\": system,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": message}]\n",
    "            })   \n",
    "\n",
    "    with Timer():\n",
    "        response = bedrock.invoke_model(\n",
    "            modelId=base_model,\n",
    "            # modelId=\"us.anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "            body=body\n",
    "        )\n",
    "\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    output_text = response_body['content'][0]['text']\n",
    "    \n",
    "    if display:\n",
    "        print(output_text)\n",
    "        print(\"\\n[모델 실행 정보] ------------------------------------------\")\n",
    "        for key in ['input_tokens', 'output_tokens']:\n",
    "            print(f\"{key} : {response_body.get('usage', {}).get(key, 0)}\")\n",
    "\n",
    "    return output_text\n",
    "\n",
    "Ret=test_invokeapi_claude(message=\"김치 볶음밥은 어떻게 만들어야 하나요?\", \n",
    "            system=\"당신은 한식 요리 전문가 입니다. 전문가로써의 정중한 답변을 해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e0574",
   "metadata": {},
   "source": [
    "### 2.3 Claude 모델의 Stream 방식 모델 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c9e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_invokeapi_claude_stream(message, system=\"\"):\n",
    "    body = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1500,\n",
    "        \"temperature\": 0.5,\n",
    "        \"system\": system,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": message}]\n",
    "    })\n",
    "\n",
    "    with Timer():\n",
    "        response = bedrock.invoke_model_with_response_stream(\n",
    "            modelId=base_model,\n",
    "            body=body\n",
    "        )\n",
    "\n",
    "    # 스트림 응답 처리\n",
    "    stream = response.get(\"body\")\n",
    "    full_text = \"\"\n",
    "\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
    "\n",
    "                # 메시지 시작 처리\n",
    "                if chunk_json.get(\"type\") == \"message_start\":\n",
    "                    continue\n",
    "\n",
    "                # 콘텐츠 블록 시작 처리\n",
    "                elif chunk_json.get(\"type\") == \"content_block_start\":\n",
    "                    continue\n",
    "\n",
    "                # 콘텐츠 블록 델타 처리 (실제 텍스트)\n",
    "                elif chunk_json.get(\"type\") == \"content_block_delta\":\n",
    "                    delta = chunk_json.get(\"delta\", {})\n",
    "                    if delta.get(\"type\") == \"text_delta\":\n",
    "                        delta_text = delta.get(\"text\", \"\")\n",
    "                        print(delta_text, end=\"\", flush=True)\n",
    "                        full_text += delta_text\n",
    "\n",
    "                # 메시지 델타 처리 (사용량 정보)\n",
    "                elif chunk_json.get(\"type\") == \"message_delta\":\n",
    "                    usage = chunk_json.get(\"usage\")\n",
    "                    if usage:\n",
    "                        print(\"\\n\\n--모델 실행 정보 --------------------------------------------------------------\")\n",
    "                        print(f\"inputTokens : {usage.get('input_tokens', 0)}\")\n",
    "                        print(f\"outputTokens : {usage.get('output_tokens', 0)}\")\n",
    "\n",
    "    return full_text\n",
    "\n",
    "# 사용 예시\n",
    "test_invokeapi_claude_stream(\n",
    "    message=\"김치 볶음밥은 어떻게 만들어야 하나요?\",\n",
    "    system=\"당신은 한식 요리 전문가 입니다. 전문가로써의 정중한 답변을 해주세요.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3777c1a",
   "metadata": {},
   "source": [
    "### 2.4 모델이 대화를 기억 하는지 테스트 \n",
    "- 기본적으로 모델은 기억 기능이 없다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5590d0f2",
   "metadata": {},
   "source": [
    "#### 1) 모델에게 내가 누구 인지 정보를 준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=test_invokeapi_claude(message=\"나는 조재구라고해 앞으로 나를 재구님 이라고 해줘!\",display=False) \n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a5ea3f",
   "metadata": {},
   "source": [
    "#### 2) 내가 누구 인지 물어본다\n",
    "- 모른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=test_invokeapi_claude(message=\"나에 이름이 무엇이라고 했지?\",display=False) \n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d0bec",
   "metadata": {},
   "source": [
    "### 2.5 모델이 대화의 내용을 기억하게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df845283",
   "metadata": {},
   "source": [
    "#### 1) 대화를 기억할 저장소를 만든다 \n",
    "- 이 저장소에 모델에게 입력한 내용과 출력한 내용을 모두 기록하도록 한다.\n",
    "- List 구조 방식으로 기록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기억 장소를 하나 만든다.\n",
    "history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972fd631",
   "metadata": {},
   "source": [
    "#### 2) 첫 번째 질문과 답변을 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d06df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 질문과 응답 \n",
    "MessageNumber1=\"나에 이름은  조재구 라고 한다. 앞으로 나를 재구님 이라고 불러줘!\"\n",
    "print(\"\\n[질문1]\",MessageNumber1)\n",
    "assistant_messages=test_invokeapi_claude(message=MessageNumber1,history=history,display=False) \n",
    "print(\"[응답 1번째:] -----------------------\")\n",
    "print(assistant_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1425b0",
   "metadata": {},
   "source": [
    "#### 3) 첫 번째 대화 내용을 기억장소에 기록한다.\n",
    "- 모델에게 요청한 정보와 응답한 메세지를 기록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd56f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 응답을 기억 장소에 저장한다. \n",
    "history.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_messages}]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65b0f13",
   "metadata": {},
   "source": [
    "#### 4) 두번째 질문을 한다. 이때 기억한 대화 정보도 함께 요청한다.\n",
    "- 아래 요청할때 history=history 라는 변수 선언이 보입니다. 위에 Helper 함수에 histoy 값을 이용해서 Body를 만들도록 하고 있습니다.\n",
    "- 이번에 응답에서 \"재구님\" 이라고 정확하게 기억하는 것을 알수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45bc6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두번째 질문을 진행한다.\n",
    "MessageNumber2=\"나에 이름이 무엇이라고 했지?\"\n",
    "print(\"\\n[질문2]\",MessageNumber2)\n",
    "print(\"[응답 2번째:] -----------------------\")\n",
    "assistant_messages=test_invokeapi_claude(message=MessageNumber1,history=history,display=False) \n",
    "print(assistant_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab931ac",
   "metadata": {},
   "source": [
    "#### 5) 두번째 대화 내용도 기억장소에 기록한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 응답을 기억 장소에 추가 저장한다. \n",
    "history.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": MessageNumber2}]})\n",
    "history.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_messages}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa9c3fc",
   "metadata": {},
   "source": [
    "#### 6) 기억 장소에 기록된 내용을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a67b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 기억장소의 내용을 표시한다.\n",
    "for item in history:\n",
    "    print(item)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8676b806",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6c0d10d",
   "metadata": {},
   "source": [
    "## 3. Converse API \n",
    "- 대화를 기능을 편리하게 제공되는 API (Chatbot선언)\n",
    "- 모델마다 다르게 Invoke API를 사용해야 하는 것과 다르게 통일된 API를 사용함 \n",
    "- https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589bf49",
   "metadata": {},
   "source": [
    "### 3.1 Parameter 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b080ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt 선언\n",
    "system_prompts = [{\"text\": \"당신은 한식 요리 전문가 입니다. 전문가로써의 정중한 답변을 해주세요.\"}]\n",
    "\n",
    "# 기억 기능을 위한 연속적인 API 호출과 출력의 메세지들을 위한 \n",
    "messages = []\n",
    "\n",
    "# 기본 Inference parameters 선언 , 모델마다 값이 다를수 있음. \n",
    "inference_config = {\"temperature\": 0.5}\n",
    "\n",
    "# 추가적인 Inference parameters, 모델마다 값이 다를수 있음. \n",
    "additional_model_fields = {\"top_p\": 0.9}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499285d1",
   "metadata": {},
   "source": [
    "### 3.2 Converse API 로 모델 호출 - 첫번째 질의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e2489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 질문을 \n",
    "messages.append( \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"김치 찌개를 끓이고 싶어요! 레시피를 알려주세요\"}]\n",
    "    }\n",
    ")\n",
    "\n",
    "response = bedrock.converse(\n",
    "        modelId=base_model,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "output_message = response['output']['message']\n",
    "\n",
    "# 출력된 데이터를 첫번째 질문의 메세지 영역에 추가 한다.\n",
    "messages.append(output_message)\n",
    "print(output_message['content'][0]['text'])\n",
    "\n",
    "print(\"------------------ MetaData ----------------------------------\")\n",
    "Usage=response.get(\"usage\")\n",
    "for key, value in Usage.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1048a17c",
   "metadata": {},
   "source": [
    "### 3.3 Converse API 로 모델 호출 - 두번째 질의\n",
    "- 주의 : 모델 실행후 Input 토큰수를 확인해야함,\n",
    "- 첫번째 모델에게 입력/출력 값을 두번째 모델의 입력으로 전달함.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    Input[첫 번째 질문] --> Model1[모델 호출]\n",
    "    Model1 --> Output1[첫 번째 모델 출력]\n",
    "    \n",
    "    Input --> Combine[첫번째 질문 + 답 + 두번째 질문 입력 결합 ]\n",
    "    Output1 --> Combine\n",
    "    \n",
    "    Combine --> Model2[두 번째 모델]\n",
    "    Model2 --> FinalOutput[최종 출력]\n",
    "    \n",
    "    style Input fill:#42a5f5,color:#fff\n",
    "    style Model1 fill:#ab47bc,color:#fff\n",
    "    style Output1 fill:#ffa726,color:#000\n",
    "    style Combine fill:#ffee58,color:#000\n",
    "    style Model2 fill:#ab47bc,color:#fff\n",
    "    style FinalOutput fill:#66bb6a,color:#fff\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞선 메세지에 2번째 질문을 추가한다. \n",
    "messages.append(  \n",
    "   {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"이 레시피에 들이가는 영양성분을 알려주세요\"}]\n",
    "    }\n",
    ")\n",
    "\n",
    "response = bedrock.converse(\n",
    "        modelId=base_model,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "output_message = response['output']['message']\n",
    "\n",
    "# 출력된 데이터를 메세지 영역에 추가 한다.\n",
    "messages.append(output_message)\n",
    "print(output_message['content'][0]['text'])\n",
    "\n",
    "print(\"------------------ MetaData ----------------------------------\")\n",
    "Usage=response.get(\"usage\")\n",
    "for key, value in Usage.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95947c71",
   "metadata": {},
   "source": [
    "### 3.4 대화 기록 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f66f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in messages:\n",
    "            print(f\"Role: {message['role']}\")\n",
    "            print(\"-----------------\")\n",
    "            for content in message['content']:\n",
    "                print(f\"Text: {content['text']}\")\n",
    "                print(\"-\" * 40)\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fd3653",
   "metadata": {},
   "source": [
    "### 3.4 Converse stream API 사용 \n",
    "- Strem 형식의 출력을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞선 메세지에 3번째 질문을 추가한다. \n",
    "messages.append(  \n",
    "   {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"지금까지 내용을 정리해줘\"}]\n",
    "    }\n",
    ")\n",
    "\n",
    "response = bedrock.converse_stream(\n",
    "        modelId=base_model,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "# Strem 형식의 출력 \n",
    "stream = response.get('stream')\n",
    "if stream:\n",
    "    for event in stream:\n",
    "\n",
    "        if 'messageStart' in event:\n",
    "            print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "        if 'contentBlockDelta' in event:\n",
    "            print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "        if 'messageStop' in event:\n",
    "            print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "        if 'metadata' in event:\n",
    "            metadata = event['metadata']\n",
    "            if 'usage' in metadata:\n",
    "                print(\"\\nToken usage\")\n",
    "                print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                print(\n",
    "                    f\":Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                print(f\":Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "            if 'metrics' in event['metadata']:\n",
    "                print(\n",
    "                    f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09aaf1d",
   "metadata": {},
   "source": [
    "## 4. 프롬프트 종류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45603d8",
   "metadata": {},
   "source": [
    "### 4.1 프롬프트 실행을 위한 Model 함수 (Helper 함수) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36664afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelInvoke(user_messages, system_prompt=\"\", inference_config={\"temperature\": 0.5}, additional_model_fields={\"top_k\": 200},modelId=base_model):\n",
    "    # 메시지 구성\n",
    "    messages = [{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": [{\"text\": user_messages}]\n",
    "    }]\n",
    "\n",
    "    # 시스템 프롬프트가 있는 경우에만 추가\n",
    "    if system_prompt:\n",
    "        system_prompts = [{\"text\": system_prompt}]\n",
    "        response = bedrock.converse_stream(\n",
    "            modelId=modelId,\n",
    "            messages=messages,\n",
    "            system=system_prompts,\n",
    "            inferenceConfig=inference_config,\n",
    "            additionalModelRequestFields=additional_model_fields\n",
    "        )\n",
    "    else:\n",
    "        response = bedrock.converse_stream(\n",
    "            modelId=modelId, \n",
    "            messages=messages,\n",
    "            inferenceConfig=inference_config,\n",
    "            additionalModelRequestFields=additional_model_fields\n",
    "        )\n",
    "\n",
    "    # Stream 형식의 출력\n",
    "    stream = response.get('stream')\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "            if 'contentBlockDelta' in event:\n",
    "                print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "            if 'metadata' in event:\n",
    "                print(\"\\n---------- MetaData --------------\")\n",
    "                metadata = event['metadata']\n",
    "\n",
    "                if 'usage' in metadata:\n",
    "                    print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                    print(f\"Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                    print(f\"Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                if 'metrics' in event['metadata']:\n",
    "                    print(f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "                print(\"---------- MetaData --------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5551fdd",
   "metadata": {},
   "source": [
    "### 4.3 System prompt를 함께 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelInvoke(user_messages=\"짜장면 요리법 알려줘\",\n",
    "            system_prompt=\"중식당 사장님으로써 성실한 조언을 해야 합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f22f6",
   "metadata": {},
   "source": [
    "### 4.4 Inference parameter를 함께 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76493a35",
   "metadata": {},
   "source": [
    "- temperature 값을 1 로 높일수록 다양한 아이디어를 생성하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09436730",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelInvoke(user_messages=\"토끼와 거북이가 경주하는 모습을 한문장에 6줄이내로 표현해줘!\",\n",
    "            system_prompt=\"유치원 선생님으로써 친절하게 이야기해줘!\",\n",
    "            inference_config={\"temperature\": 1.0},\n",
    "            additional_model_fields={\"top_k\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55ad070",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelInvoke(user_messages=\"LLM에서 temperature를 활용한 예문을 알려줘!\",\n",
    "            inference_config={\"temperature\": 0.1},\n",
    "            additional_model_fields={\"top_k\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d138bd6d",
   "metadata": {},
   "source": [
    "## 5. 도구 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b8472",
   "metadata": {},
   "source": [
    "### 5.1 Tool 선언 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 현재 시간을 반환하는 함수 정의 (Funtion Tool)\n",
    "def get_current_time():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e516eff",
   "metadata": {},
   "source": [
    "### 5.2 Tool 과 Messagess를 Model에게 전달 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool을 정의함.\n",
    "tools = [\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"get_current_time\",\n",
    "            \"description\": \"현재 시간을 반환합니다\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {},\n",
    "                    \"required\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# 메시지 정의\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"현재 시간이 몇 시인가요?\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Converse API 호출 하면서 Tool 요청 \n",
    "response = bedrock.converse(\n",
    "    modelId=base_model,\n",
    "    messages=messages,\n",
    "    toolConfig={\"tools\": tools}\n",
    ")\n",
    "\n",
    "# 응답 처리\n",
    "output_message = response['output']['message']\n",
    "print(\"------------------ Tool 요청이 있는 메세지, 'toolUse' 확인 ----------------------------------\")\n",
    "print(output_message)\n",
    "print(\"---------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e973250c",
   "metadata": {},
   "source": [
    "### 5.2 모델의 Tool사용 요청을 받아 Tool실행후 모델에게 전달 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d95f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 응답으로 부터 Tool 사용('toolUse') 요청이 있는지 확인하고 tool요청이 있으면 funtion tool을 실행한다. \n",
    "if 'content' in output_message:\n",
    "    for content in output_message['content']:\n",
    "        if 'toolUse' in content:\n",
    "            tool_use = content['toolUse']\n",
    "            tool_name = tool_use['name']\n",
    "\n",
    "            # 현재 시간을 확인하는 Funtion Tool 실행\n",
    "            if tool_name == \"get_current_time\":\n",
    "                current_time = get_current_time()\n",
    "\n",
    "                # 이전 대화 목록에 Tool 실행(현재날짜와 시간정보)결과를 메시지에 추가\n",
    "                messages.append(output_message)\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"toolResult\": {\n",
    "                                \"toolUseId\": tool_use['toolUseId'],\n",
    "                                \"content\": [{\"text\": current_time}]\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                })\n",
    "\n",
    "                # 모델에게 Tool 실행 결과와 대화기록을 함께 전달한다. \n",
    "                final_response = bedrock.converse(\n",
    "                    modelId=base_model,\n",
    "                    messages=messages,\n",
    "                    toolConfig={\"tools\": tools}\n",
    "                )\n",
    "\n",
    "                # 모델의 최종 응답 출력\n",
    "                final_content = final_response['output']['message']['content'][0]['text']\n",
    "                print(final_content)\n",
    "        else:\n",
    "            # Tool 사용 없이 직접 응답한 경우\n",
    "            print(content['text'])\n",
    "\n",
    "print(\"\\n------------------ 전체 대화 내용 확인  ----------------------------------\")\n",
    "for item in messages:\n",
    "    print(item)\n",
    "print(\"\\n\")\n",
    "print(\"---------------------------------------------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
